{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original code contributions by Dipanjan Sarkar\n",
    "#Follow the link https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html\n",
    "\n",
    "#CBOW model architecture tries to predict current target word ( center word) based on source context words (surrounding words). \n",
    "\n",
    "#CONTINUOUS BAG OF WORDS MODEL STEPS\n",
    "\n",
    "#Build the corpus vocabulary\n",
    "#Build a CBOW (context, target) generator\n",
    "#Build the CBOW model architecture\n",
    "#Train the Model\n",
    "#Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "%pprint off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 80\n",
      "Vocabulary Sample: [('not', 1), ('to', 2), ('they', 3), ('the', 4), ('be', 5), ('have', 6), ('seen', 7), ('but', 8), ('whether', 9), ('fault', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Laurence Sterne, “The Life and Opinions of Tristram Shandy.” 107 words.\n",
    "corpus = [\"The French are certainly misunderstood:- but whether the fault is theirs, in not sufficiently explaining themselves, or speaking with that exact limitation and precision which one would expect on a point of such importance, and which, moreover, is so likely to be contested by us — or whether the fault may not be altogether on our side, in not understanding their language always so critically as to know “what they would be at” — I shall not decide; but ‘tis evident to me, when they affirm, “That they who have seen Paris, have seen every thing,” they must mean to speak of those who have seen it by day-light.\"]\n",
    "\n",
    "tokenizer = text.Tokenizer() #Tokenizer instance\n",
    "\n",
    "#Fit the tokenizer object on the corpus\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "#create a dictionary\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "#Build corpus vocabulary\n",
    "\n",
    "#The PAD term is to pad context words to a fixed length if needed.\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "#Exchange key and values and store in id2word for reverse mapping\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "#Each word is mapped to a number and stored as a list of list\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "#print(wids)\n",
    "\n",
    "#Size of the vocabulary\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "#Embedding size\n",
    "embed_size = 100\n",
    "\n",
    "#context window size\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a CBOW (context, target) generator\n",
    "\n",
    "#Function that accepts the corpus in terms of sequence, size of window and vocabulary size as inputs\n",
    "#and yields target word with the surrounding context words\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    \n",
    "    #Context length is twice the window size specified by the user\n",
    "    context_length = window_size*2\n",
    "    \n",
    "    #For every word in corpus\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words) #Obtain the length of words\n",
    "        \n",
    "        #For every word obtain the context_words and its label\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            #Pad sequence if required and yield sequence and labels\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = tensorflow.keras.utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['the', 'french', 'certainly', 'misunderstood'] -> Target (Y): are\n",
      "Context (X): ['french', 'are', 'misunderstood', 'but'] -> Target (Y): certainly\n",
      "Context (X): ['are', 'certainly', 'but', 'whether'] -> Target (Y): misunderstood\n",
      "Context (X): ['certainly', 'misunderstood', 'whether', 'the'] -> Target (Y): but\n",
      "Context (X): ['misunderstood', 'but', 'the', 'fault'] -> Target (Y): whether\n",
      "Context (X): ['but', 'whether', 'fault', 'is'] -> Target (Y): the\n",
      "Context (X): ['whether', 'the', 'is', 'theirs'] -> Target (Y): fault\n",
      "Context (X): ['the', 'fault', 'theirs', 'in'] -> Target (Y): is\n",
      "Context (X): ['fault', 'is', 'in', 'not'] -> Target (Y): theirs\n",
      "Context (X): ['is', 'theirs', 'not', 'sufficiently'] -> Target (Y): in\n",
      "Context (X): ['theirs', 'in', 'sufficiently', 'explaining'] -> Target (Y): not\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results interpretation (first instance)\n",
    "#The first four words are the context words. In this, the first two are words before and the last two are words after\n",
    "#Then the resulting target center word is \"are\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            8000      \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                8080      \n",
      "=================================================================\n",
      "Total params: 16,080\n",
      "Trainable params: 16,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Build the CBOW Model Architecture\n",
    "\n",
    "#Import necessary libraries\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "\n",
    "#Input context words passed to embedding layer (initialised with random weights)\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "\n",
    "#Average out the word embeddings in lambda layer\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "\n",
    "#Averaged context embedding is passed to a dense softmax layer which predicts the target word\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary (8000 parameters (vocab_size = 80) * (embed_size = 100) are to be trained)\n",
    "#No parameters are to be trained at the lambda layer\n",
    "#80 vocab * 100 embed_size = 8000 + 80(bias) = 8080 parmaters are the output \n",
    "\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 487.5397434234619\n",
      "Epoch: 2 \tLoss: 483.0509033203125\n",
      "Epoch: 3 \tLoss: 478.81833934783936\n",
      "Epoch: 4 \tLoss: 474.2853865623474\n"
     ]
    }
   ],
   "source": [
    "#Train the Model for few epochs. train_on_batch - runs a single gradient update on a single batch of data.\n",
    "for epoch in range(1, 5):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.044938</td>\n",
       "      <td>-0.008269</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>0.078354</td>\n",
       "      <td>0.036937</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>0.054591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>-0.040447</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.029801</td>\n",
       "      <td>0.027725</td>\n",
       "      <td>-0.008629</td>\n",
       "      <td>0.075807</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>-0.037415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>-0.037934</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.049663</td>\n",
       "      <td>0.025464</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>-0.012186</td>\n",
       "      <td>-0.032130</td>\n",
       "      <td>0.041848</td>\n",
       "      <td>-0.003856</td>\n",
       "      <td>-0.025700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013786</td>\n",
       "      <td>-0.020892</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>-0.058641</td>\n",
       "      <td>-0.051232</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.062866</td>\n",
       "      <td>-0.010039</td>\n",
       "      <td>-0.020018</td>\n",
       "      <td>0.010626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.025242</td>\n",
       "      <td>-0.012060</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.077378</td>\n",
       "      <td>0.024225</td>\n",
       "      <td>0.046751</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>-0.045397</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069822</td>\n",
       "      <td>-0.039655</td>\n",
       "      <td>-0.030293</td>\n",
       "      <td>-0.019784</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>-0.035284</td>\n",
       "      <td>-0.095962</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>0.000919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0.034076</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>-0.060514</td>\n",
       "      <td>-0.046412</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.045996</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004562</td>\n",
       "      <td>0.058907</td>\n",
       "      <td>-0.006440</td>\n",
       "      <td>0.047870</td>\n",
       "      <td>0.067822</td>\n",
       "      <td>-0.035752</td>\n",
       "      <td>-0.030949</td>\n",
       "      <td>-0.015416</td>\n",
       "      <td>-0.036782</td>\n",
       "      <td>0.093722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>0.024941</td>\n",
       "      <td>-0.014621</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.009571</td>\n",
       "      <td>0.015912</td>\n",
       "      <td>0.051762</td>\n",
       "      <td>0.033805</td>\n",
       "      <td>0.017998</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>-0.020380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064365</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.025919</td>\n",
       "      <td>-0.015937</td>\n",
       "      <td>-0.017667</td>\n",
       "      <td>-0.034889</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>-0.019541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "to    0.044938 -0.008269  0.010627  0.026042  0.002331  0.006940  0.078354   \n",
       "they -0.037934  0.002994  0.049663  0.025464 -0.026644 -0.012186 -0.032130   \n",
       "the   0.025242 -0.012060  0.015147  0.077378  0.024225  0.046751  0.031603   \n",
       "be    0.034076  0.013433 -0.004254  0.015812 -0.060514 -0.046412  0.015892   \n",
       "have  0.024941 -0.014621 -0.053798 -0.009571  0.015912  0.051762  0.033805   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "to    0.036937 -0.012378  0.054591  ... -0.014029 -0.040447  0.003625   \n",
       "they  0.041848 -0.003856 -0.025700  ... -0.013786 -0.020892  0.014673   \n",
       "the   0.021123 -0.045397  0.019390  ...  0.069822 -0.039655 -0.030293   \n",
       "be    0.045996  0.020714  0.039490  ... -0.004562  0.058907 -0.006440   \n",
       "have  0.017998  0.015519 -0.020380  ... -0.064365  0.027920  0.011062   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "to    0.029801  0.027725 -0.008629  0.075807  0.013590  0.005600 -0.037415  \n",
       "they -0.058641 -0.051232  0.009474  0.062866 -0.010039 -0.020018  0.010626  \n",
       "the  -0.019784 -0.057372 -0.028622 -0.035284 -0.095962  0.044647  0.000919  \n",
       "be    0.047870  0.067822 -0.035752 -0.030949 -0.015416 -0.036782  0.093722  \n",
       "have  0.025919 -0.015937 -0.017667 -0.034889  0.004214  0.031744 -0.019541  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word embeddings for the vocabulary\n",
    "\n",
    "weights = cbow.get_weights()[0] #Word embedding of PAD\n",
    "weights = weights[1:] #Exclude word embedding of PAD\n",
    "print(weights.shape) # 79 (80-1) vocabulary \n",
    "\n",
    "#Convert the weights to a dataframe for each of the word\n",
    "#A single row shows the word embedding done in 100 dimensions by CBOW model \n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 79)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'french': ['our', 'altogether', 'misunderstood', 'light', 'but'],\n",
       " 'sufficiently': ['understanding',\n",
       "  'theirs',\n",
       "  'contested',\n",
       "  'expect',\n",
       "  'importance'],\n",
       " 'paris': ['it', 'every', 'point', 'by', 'us']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for context similarity based on euclidean distances\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape) # (79,79)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['french','sufficiently','paris']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result interpretation\n",
    "#Some words are contextually similar while some are not even with limited vocabulary of 79 words\n",
    "\n",
    "#Try the CBOW model with slightly larger corpus\n",
    "#Download Alice in Wonderland from Project Gutenberg and store it in current working directory as Alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the file\n",
    "f = open('Alice.txt','r', encoding = 'utf-8')\n",
    "\n",
    "#Read line by line\n",
    "alice = f.readlines()\n",
    "\n",
    "#Remove all \\n elements \n",
    "alRem = list(map(lambda s: s.strip(), alice ))\n",
    "\n",
    "#Check total number of elements\n",
    "print(\"Total number of list elements: \", len(alRem)) #3773 elements\n",
    "\n",
    "#List comprehension to remove empty strings\n",
    "alNoEmpStr = [i for i in alRem if i]\n",
    "\n",
    "#After removing empty strings, length of the list\n",
    "print(\"Total length of list after removing empty strings: \", len(alNoEmpStr)) #2815 elements\n",
    "\n",
    "#First sentence in Chapter One is 34th element and the last sentence is 2508th element\n",
    "#prepare a corpus based on these element numbers\n",
    "alCorpus = alNoEmpStr[34:2508]\n",
    "\n",
    "print(\"First two elements: \", alCorpus[0:1]) #First two elements in the list\n",
    "print(\"Last two elements: \", alCorpus[-2:]) #Last two elements in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3054\n",
      "Vocabulary Sample: [('the', 1), ('”', 2), ('and', 3), ('to', 4), ('a', 5), ('she', 6), ('of', 7), ('it', 8), ('said', 9), ('alice', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Now corpus is ready for CBOW Modeling\n",
    "tokenizer = text.Tokenizer() #Tokenizer instance\n",
    "\n",
    "#Fit the tokenizer object on the Alice corpus\n",
    "tokenizer.fit_on_texts(alCorpus)\n",
    "\n",
    "#create a dictionary\n",
    "word2idAl = tokenizer.word_index\n",
    "\n",
    "#Build corpus vocabulary\n",
    "\n",
    "#The PAD term is to pad context words to a fixed length if needed.\n",
    "word2idAl['PAD'] = 0\n",
    "\n",
    "#Exchange key and values and store in id2word for reverse mapping\n",
    "id2wordAl = {v:k for k, v in word2idAl.items()}\n",
    "\n",
    "#Each word is mapped to a number and stored as a list of list\n",
    "widsAl = [[word2idAl[w] for w in text.text_to_word_sequence(doc)] for doc in alCorpus]\n",
    "#print(wids)\n",
    "\n",
    "#Size of the vocabulary\n",
    "vocab_size_alice = len(word2idAl)\n",
    "\n",
    "#Embedding size\n",
    "embed_size = 100\n",
    "\n",
    "#context window size\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size_alice) #3054\n",
    "print('Vocabulary Sample:', list(word2idAl.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['alice', 'was', 'to', 'get'] -> Target (Y): beginning\n",
      "Context (X): ['was', 'beginning', 'get', 'very'] -> Target (Y): to\n",
      "Context (X): ['beginning', 'to', 'very', 'tired'] -> Target (Y): get\n",
      "Context (X): ['to', 'get', 'tired', 'of'] -> Target (Y): very\n",
      "Context (X): ['get', 'very', 'of', 'sitting'] -> Target (Y): tired\n",
      "Context (X): ['very', 'tired', 'sitting', 'by'] -> Target (Y): of\n",
      "Context (X): ['tired', 'of', 'by', 'her'] -> Target (Y): sitting\n",
      "Context (X): ['of', 'sitting', 'her', 'sister'] -> Target (Y): by\n",
      "Context (X): ['sitting', 'by', 'sister', 'on'] -> Target (Y): her\n",
      "Context (X): ['by', 'her', 'on', 'the'] -> Target (Y): sister\n",
      "Context (X): ['bank', 'and', 'having', 'nothing'] -> Target (Y): of\n"
     ]
    }
   ],
   "source": [
    "#Call the generate_context_word_pairs function created earlier\n",
    "\n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=widsAl, window_size=window_size, vocab_size=vocab_size_alice):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2wordAl[w] for w in x[0]], '-> Target (Y):', id2wordAl[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CBOW architecture\n",
    "cbowAl = Sequential()\n",
    "\n",
    "#Input context words passed to embedding layer (initialised with random weights)\n",
    "cbowAl.add(Embedding(input_dim=vocab_size_alice, output_dim=embed_size, input_length=window_size*2))\n",
    "\n",
    "#Average out the word embeddings in lambda layer\n",
    "cbowAl.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "\n",
    "#Averaged context embedding is passed to a dense softmax layer which predicts the target word\n",
    "cbowAl.add(Dense(vocab_size_alice, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "cbowAl.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            305400    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3054)              308454    \n",
      "=================================================================\n",
      "Total params: 613,854\n",
      "Trainable params: 613,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cbowAl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of parameters in the embedding layer = 3054 (Vocabulary of alice corpus) * 100 dimensions = 305400\n",
    "#Number of parameters in the dense output layer = 3054 * 100 (dim) + 3054 (bias) = 308454 parameters\n",
    "#All 613854 parameters are trainable compared to 16080 parameters in earlier model of vocab 80 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 186648.92244170606\n",
      "Epoch: 2 \tLoss: 229101.56220288947\n",
      "Epoch: 3 \tLoss: 233897.94276710693\n",
      "Epoch: 4 \tLoss: 235240.0928955432\n",
      "Epoch: 5 \tLoss: 241231.58937511826\n",
      "Epoch: 6 \tLoss: 244479.49974279804\n",
      "Epoch: 7 \tLoss: 246508.15938479546\n",
      "Epoch: 8 \tLoss: 246198.8009637592\n",
      "Epoch: 9 \tLoss: 250720.9576856495\n",
      "Epoch: 10 \tLoss: 252279.28531993495\n"
     ]
    }
   ],
   "source": [
    "#Train the model for 10 epochs. Each epoch takes at least 5 minutes. Total training time approximately 50 minutes\n",
    "for epoch in range(1, 11):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=widsAl, window_size=window_size, vocab_size=vocab_size_alice):\n",
    "        i += 1\n",
    "        loss += cbowAl.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3053, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>1.387312</td>\n",
       "      <td>1.059913</td>\n",
       "      <td>1.399809</td>\n",
       "      <td>-1.705998</td>\n",
       "      <td>-1.438428</td>\n",
       "      <td>1.978453</td>\n",
       "      <td>-1.728255</td>\n",
       "      <td>-1.331693</td>\n",
       "      <td>-1.273764</td>\n",
       "      <td>1.209744</td>\n",
       "      <td>...</td>\n",
       "      <td>1.229479</td>\n",
       "      <td>0.988443</td>\n",
       "      <td>-1.452939</td>\n",
       "      <td>1.412286</td>\n",
       "      <td>1.410160</td>\n",
       "      <td>-1.139220</td>\n",
       "      <td>1.437299</td>\n",
       "      <td>-1.100276</td>\n",
       "      <td>-1.148705</td>\n",
       "      <td>1.783028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1.092667</td>\n",
       "      <td>1.113216</td>\n",
       "      <td>1.259892</td>\n",
       "      <td>-1.383837</td>\n",
       "      <td>-0.690532</td>\n",
       "      <td>0.910162</td>\n",
       "      <td>-1.399434</td>\n",
       "      <td>-1.341926</td>\n",
       "      <td>-0.792996</td>\n",
       "      <td>1.580792</td>\n",
       "      <td>...</td>\n",
       "      <td>1.413779</td>\n",
       "      <td>0.857467</td>\n",
       "      <td>-1.270074</td>\n",
       "      <td>1.743577</td>\n",
       "      <td>0.938099</td>\n",
       "      <td>-1.457112</td>\n",
       "      <td>1.127715</td>\n",
       "      <td>-1.331147</td>\n",
       "      <td>-1.084189</td>\n",
       "      <td>1.176422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1.298293</td>\n",
       "      <td>1.723328</td>\n",
       "      <td>1.397377</td>\n",
       "      <td>-1.622204</td>\n",
       "      <td>-1.253475</td>\n",
       "      <td>1.313388</td>\n",
       "      <td>-1.614611</td>\n",
       "      <td>-1.379891</td>\n",
       "      <td>-1.160793</td>\n",
       "      <td>1.447603</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393684</td>\n",
       "      <td>1.211561</td>\n",
       "      <td>-1.333187</td>\n",
       "      <td>1.819563</td>\n",
       "      <td>1.432811</td>\n",
       "      <td>-1.147800</td>\n",
       "      <td>1.366220</td>\n",
       "      <td>-1.685795</td>\n",
       "      <td>-1.588862</td>\n",
       "      <td>1.088095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.334192</td>\n",
       "      <td>1.155898</td>\n",
       "      <td>1.031305</td>\n",
       "      <td>-1.776452</td>\n",
       "      <td>-1.643120</td>\n",
       "      <td>1.327358</td>\n",
       "      <td>-1.538120</td>\n",
       "      <td>-1.119446</td>\n",
       "      <td>-0.803385</td>\n",
       "      <td>1.446294</td>\n",
       "      <td>...</td>\n",
       "      <td>1.283289</td>\n",
       "      <td>1.071023</td>\n",
       "      <td>-1.479391</td>\n",
       "      <td>1.716610</td>\n",
       "      <td>0.964271</td>\n",
       "      <td>-1.334887</td>\n",
       "      <td>1.029508</td>\n",
       "      <td>-1.226262</td>\n",
       "      <td>-1.563774</td>\n",
       "      <td>1.358557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>1.064586</td>\n",
       "      <td>1.454698</td>\n",
       "      <td>1.035282</td>\n",
       "      <td>-1.684532</td>\n",
       "      <td>-1.141873</td>\n",
       "      <td>1.150333</td>\n",
       "      <td>-1.464615</td>\n",
       "      <td>-0.815316</td>\n",
       "      <td>-1.480658</td>\n",
       "      <td>1.208726</td>\n",
       "      <td>...</td>\n",
       "      <td>1.460051</td>\n",
       "      <td>0.836496</td>\n",
       "      <td>-1.743281</td>\n",
       "      <td>1.628383</td>\n",
       "      <td>1.125051</td>\n",
       "      <td>-1.818674</td>\n",
       "      <td>1.212635</td>\n",
       "      <td>-0.905509</td>\n",
       "      <td>-1.077633</td>\n",
       "      <td>1.768557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "”    1.387312  1.059913  1.399809 -1.705998 -1.438428  1.978453 -1.728255   \n",
       "and  1.092667  1.113216  1.259892 -1.383837 -0.690532  0.910162 -1.399434   \n",
       "to   1.298293  1.723328  1.397377 -1.622204 -1.253475  1.313388 -1.614611   \n",
       "a    1.334192  1.155898  1.031305 -1.776452 -1.643120  1.327358 -1.538120   \n",
       "she  1.064586  1.454698  1.035282 -1.684532 -1.141873  1.150333 -1.464615   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       "”   -1.331693 -1.273764  1.209744  ...  1.229479  0.988443 -1.452939   \n",
       "and -1.341926 -0.792996  1.580792  ...  1.413779  0.857467 -1.270074   \n",
       "to  -1.379891 -1.160793  1.447603  ...  1.393684  1.211561 -1.333187   \n",
       "a   -1.119446 -0.803385  1.446294  ...  1.283289  1.071023 -1.479391   \n",
       "she -0.815316 -1.480658  1.208726  ...  1.460051  0.836496 -1.743281   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "”    1.412286  1.410160 -1.139220  1.437299 -1.100276 -1.148705  1.783028  \n",
       "and  1.743577  0.938099 -1.457112  1.127715 -1.331147 -1.084189  1.176422  \n",
       "to   1.819563  1.432811 -1.147800  1.366220 -1.685795 -1.588862  1.088095  \n",
       "a    1.716610  0.964271 -1.334887  1.029508 -1.226262 -1.563774  1.358557  \n",
       "she  1.628383  1.125051 -1.818674  1.212635 -0.905509 -1.077633  1.768557  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word embeddings for the vocabulary\n",
    "\n",
    "weightsAl = cbowAl.get_weights()[0] #Word embedding of PAD\n",
    "weightsAl = weightsAl[1:] #Exclude word embedding of PAD\n",
    "print(weightsAl.shape) # 3053 (3054-1) vocabulary \n",
    "\n",
    "#Convert the weights to a dataframe for each of the word\n",
    "#A single row shows the word embedding done in 100 dimensions by CBOW model\n",
    "\n",
    "#Shape of dataframe (3053,100)\n",
    "\n",
    "aliceDF = pd.DataFrame(weightsAl, index=list(id2wordAl.values())[1:])\n",
    "aliceDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3053, 3053)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tired': ['immediately', 'full', '’em', 'free', 'dish'],\n",
       " 'well': ['soon', 'something', 'heard', '“it', 'half'],\n",
       " 'bats': ['sending', 'she’ll', 'proceed', 'feelings', 'miles'],\n",
       " 'alice': ['with', '“i', 'as', 'but', 'his'],\n",
       " 'croquet': ['play', 'shouting', 'flat', 'trouble', 'also'],\n",
       " 'cheshire': ['croquet', 'trouble', 'waving', 'altogether', '“sure'],\n",
       " 'hatter': ['duchess', 'gryphon', 'cat', 'indeed', 'making'],\n",
       " 'down': ['up', 'off', 'into', 'about', 'an'],\n",
       " 'flamingo': ['latin', 'yet—oh', 'kissed', 'daughter', 'ordering'],\n",
       " 'adventures': ['repeating', 'changes', 'neither', 'daughter', 'sister’s']}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for context similarity based on euclidean distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix_alice = euclidean_distances(weightsAl)\n",
    "print(distance_matrix_alice.shape) # (3053,3053)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words_alice = {search_term: [id2wordAl[idx] for idx in distance_matrix_alice[word2idAl[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['tired','well','bats','alice','croquet',\n",
    "                                       'cheshire','hatter','down','flamingo','adventures']}\n",
    "\n",
    "similar_words_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context similarities can be found for words like play for croquet.\n",
    "#Preprocessing the text and more training shall yield more contextually relevant results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

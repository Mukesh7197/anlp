{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original code contributions by Dipanjan Sarkar\n",
    "#Follow the link https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html\n",
    "\n",
    "#CBOW model architecture tries to predict current target word ( center word) based on source context words (surrounding words).\n",
    "\n",
    "#Own contribution - (i) predict the target word based on context word and interpretation of prediction\n",
    "#                    (ii) loss vs epoch plots\n",
    "\n",
    "#CONTINUOUS BAG OF WORDS MODEL STEPS\n",
    "\n",
    "#Build the corpus vocabulary\n",
    "#Build a CBOW (context, target) generator\n",
    "#Build the CBOW model architecture\n",
    "#Train the Model\n",
    "#Test the Model\n",
    "#Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%pprint off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 80\n",
      "Vocabulary Sample: [('not', 1), ('to', 2), ('they', 3), ('the', 4), ('be', 5), ('have', 6), ('seen', 7), ('but', 8), ('whether', 9), ('fault', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Laurence Sterne, “The Life and Opinions of Tristram Shandy.” 107 words.\n",
    "corpus = [\"The French are certainly misunderstood:- but whether the fault is theirs, in not sufficiently explaining themselves, or speaking with that exact limitation and precision which one would expect on a point of such importance, and which, moreover, is so likely to be contested by us — or whether the fault may not be altogether on our side, in not understanding their language always so critically as to know “what they would be at” — I shall not decide; but ‘tis evident to me, when they affirm, “That they who have seen Paris, have seen every thing,” they must mean to speak of those who have seen it by day-light.\"]\n",
    "\n",
    "tokenizer = text.Tokenizer() #Tokenizer instance\n",
    "\n",
    "#Fit the tokenizer object on the corpus\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "#create a dictionary\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "#Build corpus vocabulary\n",
    "\n",
    "#The PAD term is to pad context words to a fixed length if needed.\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "#Exchange key and values and store in id2word for reverse mapping\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "#Each word is mapped to a number and stored as a list of list\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "#print(wids)\n",
    "\n",
    "#Size of the vocabulary\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "#Embedding size\n",
    "embed_size = 100\n",
    "\n",
    "#context window size\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not': 1, 'to': 2, 'they': 3, 'the': 4, 'be': 5, 'have': 6, 'seen': 7, 'but': 8, 'whether': 9, 'fault': 10, 'is': 11, 'in': 12, 'or': 13, 'and': 14, 'which': 15, 'would': 16, 'on': 17, 'of': 18, 'so': 19, 'by': 20, '—': 21, 'who': 22, 'french': 23, 'are': 24, 'certainly': 25, 'misunderstood': 26, 'theirs': 27, 'sufficiently': 28, 'explaining': 29, 'themselves': 30, 'speaking': 31, 'with': 32, 'that': 33, 'exact': 34, 'limitation': 35, 'precision': 36, 'one': 37, 'expect': 38, 'a': 39, 'point': 40, 'such': 41, 'importance': 42, 'moreover': 43, 'likely': 44, 'contested': 45, 'us': 46, 'may': 47, 'altogether': 48, 'our': 49, 'side': 50, 'understanding': 51, 'their': 52, 'language': 53, 'always': 54, 'critically': 55, 'as': 56, 'know': 57, '“what': 58, 'at”': 59, 'i': 60, 'shall': 61, 'decide': 62, '‘tis': 63, 'evident': 64, 'me': 65, 'when': 66, 'affirm': 67, '“that': 68, 'paris': 69, 'every': 70, 'thing': 71, '”': 72, 'must': 73, 'mean': 74, 'speak': 75, 'those': 76, 'it': 77, 'day': 78, 'light': 79, 'PAD': 0}\n"
     ]
    }
   ],
   "source": [
    "#word2id dictionary. Note 'PAD' is initialised to zero\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'not', 2: 'to', 3: 'they', 4: 'the', 5: 'be', 6: 'have', 7: 'seen', 8: 'but', 9: 'whether', 10: 'fault', 11: 'is', 12: 'in', 13: 'or', 14: 'and', 15: 'which', 16: 'would', 17: 'on', 18: 'of', 19: 'so', 20: 'by', 21: '—', 22: 'who', 23: 'french', 24: 'are', 25: 'certainly', 26: 'misunderstood', 27: 'theirs', 28: 'sufficiently', 29: 'explaining', 30: 'themselves', 31: 'speaking', 32: 'with', 33: 'that', 34: 'exact', 35: 'limitation', 36: 'precision', 37: 'one', 38: 'expect', 39: 'a', 40: 'point', 41: 'such', 42: 'importance', 43: 'moreover', 44: 'likely', 45: 'contested', 46: 'us', 47: 'may', 48: 'altogether', 49: 'our', 50: 'side', 51: 'understanding', 52: 'their', 53: 'language', 54: 'always', 55: 'critically', 56: 'as', 57: 'know', 58: '“what', 59: 'at”', 60: 'i', 61: 'shall', 62: 'decide', 63: '‘tis', 64: 'evident', 65: 'me', 66: 'when', 67: 'affirm', 68: '“that', 69: 'paris', 70: 'every', 71: 'thing', 72: '”', 73: 'must', 74: 'mean', 75: 'speak', 76: 'those', 77: 'it', 78: 'day', 79: 'light', 0: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "#reverse dictionary id2word. Total words including PAD is 80\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a CBOW (context, target) generator\n",
    "\n",
    "#Function that accepts the corpus in terms of sequence, size of window and vocabulary size as inputs\n",
    "#and yields target word with the surrounding context words\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    \n",
    "    #Context length is twice the window size specified by the user\n",
    "    context_length = window_size*2\n",
    "    \n",
    "    #For every word in corpus\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words) #Obtain the length of words\n",
    "        \n",
    "        #For every word obtain the context_words and its label\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            #Pad sequence if required and yield sequence and labels\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = tensorflow.keras.utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['the', 'french', 'certainly', 'misunderstood'] -> Target (Y): are\n",
      "Context (X): ['french', 'are', 'misunderstood', 'but'] -> Target (Y): certainly\n",
      "Context (X): ['are', 'certainly', 'but', 'whether'] -> Target (Y): misunderstood\n",
      "Context (X): ['certainly', 'misunderstood', 'whether', 'the'] -> Target (Y): but\n",
      "Context (X): ['misunderstood', 'but', 'the', 'fault'] -> Target (Y): whether\n",
      "Context (X): ['but', 'whether', 'fault', 'is'] -> Target (Y): the\n",
      "Context (X): ['whether', 'the', 'is', 'theirs'] -> Target (Y): fault\n",
      "Context (X): ['the', 'fault', 'theirs', 'in'] -> Target (Y): is\n",
      "Context (X): ['fault', 'is', 'in', 'not'] -> Target (Y): theirs\n",
      "Context (X): ['is', 'theirs', 'not', 'sufficiently'] -> Target (Y): in\n",
      "Context (X): ['theirs', 'in', 'sufficiently', 'explaining'] -> Target (Y): not\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results interpretation (first instance)\n",
    "#The first four words are the context words. \n",
    "#In this, the first two are words before the center word and the last two are words after the center word\n",
    "#Then the resulting target center word is \"are\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            8000      \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                8080      \n",
      "=================================================================\n",
      "Total params: 16,080\n",
      "Trainable params: 16,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Try Sequential Neural Network based prediction (CBOW Model)\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "\n",
    "#Input context words passed to embedding layer (initialised with random weights)\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "\n",
    "#Average out the word embeddings in lambda layer\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "\n",
    "#Averaged context embedding is passed to a dense softmax layer which outputs the probabilities of each word in the vocab\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary (8000 parameters (vocab_size = 80) * (embed_size = 100) are to be trained)\n",
    "#No parameters are to be trained at the lambda layer\n",
    "#80 vocab * 100 embed_size = 8000 + 80(bias) = 8080 parmaters are the output \n",
    "\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Model for 100 epochs as vocabulary is just 80. Takes couple of minutes\n",
    "#train_on_batch - runs a single gradient update on a single batch of data.\n",
    "Loss = []\n",
    "Epoch = []\n",
    "for epoch in range(1, 100):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    Loss.append(loss)\n",
    "    Epoch.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['the', 'french', 'certainly', 'misunderstood'] -> Target (Y): are\n",
      "Context (X): ['french', 'are', 'misunderstood', 'but'] -> Target (Y): certainly\n",
      "Context (X): ['are', 'certainly', 'but', 'whether'] -> Target (Y): misunderstood\n",
      "Context (X): ['certainly', 'misunderstood', 'whether', 'the'] -> Target (Y): but\n",
      "Context (X): ['misunderstood', 'but', 'the', 'fault'] -> Target (Y): whether\n",
      "Context (X): ['but', 'whether', 'fault', 'is'] -> Target (Y): the\n",
      "Context (X): ['whether', 'the', 'is', 'theirs'] -> Target (Y): fault\n",
      "Context (X): ['the', 'fault', 'theirs', 'in'] -> Target (Y): is\n",
      "Context (X): ['fault', 'is', 'in', 'not'] -> Target (Y): theirs\n",
      "Context (X): ['is', 'theirs', 'not', 'sufficiently'] -> Target (Y): in\n",
      "Context (X): ['theirs', 'in', 'sufficiently', 'explaining'] -> Target (Y): not\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples. \n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argmax(cbow.predict([x,y]))])\n",
    "        #print('Context (X):', [id2word[w] for w in x[0]])\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "#Training for 100 epochs for a smaller corpus has yielded good prediction results. \n",
    "#Training for less than 10 epochs will result in poor predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.852154</td>\n",
       "      <td>-0.909942</td>\n",
       "      <td>0.412170</td>\n",
       "      <td>0.525520</td>\n",
       "      <td>1.155522</td>\n",
       "      <td>-0.697109</td>\n",
       "      <td>0.162528</td>\n",
       "      <td>-0.057547</td>\n",
       "      <td>0.234085</td>\n",
       "      <td>-0.342718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501566</td>\n",
       "      <td>-0.265228</td>\n",
       "      <td>-0.989959</td>\n",
       "      <td>-0.558939</td>\n",
       "      <td>0.567455</td>\n",
       "      <td>-0.287729</td>\n",
       "      <td>-1.063588</td>\n",
       "      <td>-0.642793</td>\n",
       "      <td>-0.238514</td>\n",
       "      <td>0.377450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>-0.355087</td>\n",
       "      <td>0.689162</td>\n",
       "      <td>-0.074675</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>-0.364085</td>\n",
       "      <td>0.082830</td>\n",
       "      <td>-0.587688</td>\n",
       "      <td>0.312270</td>\n",
       "      <td>-0.614695</td>\n",
       "      <td>0.783426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183202</td>\n",
       "      <td>0.134065</td>\n",
       "      <td>0.158441</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.124788</td>\n",
       "      <td>0.669325</td>\n",
       "      <td>0.381242</td>\n",
       "      <td>0.357798</td>\n",
       "      <td>0.738200</td>\n",
       "      <td>-0.562784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.140297</td>\n",
       "      <td>0.734499</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.739054</td>\n",
       "      <td>-0.701238</td>\n",
       "      <td>-0.653234</td>\n",
       "      <td>0.110845</td>\n",
       "      <td>0.206770</td>\n",
       "      <td>-0.818757</td>\n",
       "      <td>0.072044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>-1.080587</td>\n",
       "      <td>0.359994</td>\n",
       "      <td>-0.587371</td>\n",
       "      <td>-0.101311</td>\n",
       "      <td>-0.296440</td>\n",
       "      <td>0.400391</td>\n",
       "      <td>0.139627</td>\n",
       "      <td>0.491590</td>\n",
       "      <td>-0.181183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0.563303</td>\n",
       "      <td>-0.332927</td>\n",
       "      <td>-1.126917</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.421359</td>\n",
       "      <td>-0.786144</td>\n",
       "      <td>-0.266430</td>\n",
       "      <td>-0.342233</td>\n",
       "      <td>-0.132217</td>\n",
       "      <td>0.121101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305070</td>\n",
       "      <td>0.348162</td>\n",
       "      <td>-0.080301</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>-0.346357</td>\n",
       "      <td>-0.865583</td>\n",
       "      <td>-0.157885</td>\n",
       "      <td>-0.361634</td>\n",
       "      <td>0.112093</td>\n",
       "      <td>0.262043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>0.381923</td>\n",
       "      <td>-0.511404</td>\n",
       "      <td>0.446980</td>\n",
       "      <td>-0.875571</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.060669</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>-0.639367</td>\n",
       "      <td>-0.149370</td>\n",
       "      <td>-0.550834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465896</td>\n",
       "      <td>0.964836</td>\n",
       "      <td>0.733412</td>\n",
       "      <td>-0.442579</td>\n",
       "      <td>0.680833</td>\n",
       "      <td>0.328109</td>\n",
       "      <td>-0.277793</td>\n",
       "      <td>-0.107638</td>\n",
       "      <td>-0.966456</td>\n",
       "      <td>-0.339343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "to    0.852154 -0.909942  0.412170  0.525520  1.155522 -0.697109  0.162528   \n",
       "they -0.355087  0.689162 -0.074675  0.447100 -0.364085  0.082830 -0.587688   \n",
       "the   0.140297  0.734499  0.016000  0.739054 -0.701238 -0.653234  0.110845   \n",
       "be    0.563303 -0.332927 -1.126917  0.006636  0.421359 -0.786144 -0.266430   \n",
       "have  0.381923 -0.511404  0.446980 -0.875571  0.036914  0.060669  0.046296   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "to   -0.057547  0.234085 -0.342718  ... -0.501566 -0.265228 -0.989959   \n",
       "they  0.312270 -0.614695  0.783426  ...  0.183202  0.134065  0.158441   \n",
       "the   0.206770 -0.818757  0.072044  ...  0.013164 -1.080587  0.359994   \n",
       "be   -0.342233 -0.132217  0.121101  ...  0.305070  0.348162 -0.080301   \n",
       "have -0.639367 -0.149370 -0.550834  ... -0.465896  0.964836  0.733412   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "to   -0.558939  0.567455 -0.287729 -1.063588 -0.642793 -0.238514  0.377450  \n",
       "they -0.761043  0.124788  0.669325  0.381242  0.357798  0.738200 -0.562784  \n",
       "the  -0.587371 -0.101311 -0.296440  0.400391  0.139627  0.491590 -0.181183  \n",
       "be    0.006818 -0.346357 -0.865583 -0.157885 -0.361634  0.112093  0.262043  \n",
       "have -0.442579  0.680833  0.328109 -0.277793 -0.107638 -0.966456 -0.339343  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word embeddings for the vocabulary\n",
    "\n",
    "weights = cbow.get_weights()[0] #Word embedding of PAD\n",
    "weights = weights[1:] #Exclude word embedding of PAD\n",
    "print(weights.shape) # 79 (80-1) vocabulary \n",
    "\n",
    "#Convert the weights to a dataframe for each of the word\n",
    "#A single row shows the word embedding done in 100 dimensions by CBOW model \n",
    "\n",
    "weightsDF = pd.DataFrame(weights, index=list(id2word.values())[1:]) #Shape (79,100)\n",
    "weightsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss vs epoch\n",
    "epLoL = list(zip(Epoch, Loss)) #Zip the list\n",
    "epLoDF = pd.DataFrame(epLoL, columns = ['Epoch','Loss']) #Convert to a dataframe\n",
    "\n",
    "epLoDF.plot.line(x='Epoch', y= 'Loss', c='r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('CBOW Model')\n",
    "plt.show()\n",
    "\n",
    "#Initially losses are high. Loss is decreasing steadily for 100 epochs. Prediction accuracy will hence improve if trained more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try the CBOW model with slightly larger corpus\n",
    "#Download Alice in Wonderland from Project Gutenberg and store it in current working directory as Alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of list elements:  3773\n",
      "Total length of list after removing empty strings:  2815\n",
      "First two elements:  ['Alice was beginning to get very tired of sitting by her sister on the', 'bank, and of having nothing to do: once or twice she had peeped into']\n",
      "Last two elements:  ['their simple joys, remembering her own child-life, and the happy summer', 'days.']\n"
     ]
    }
   ],
   "source": [
    "#Open the file\n",
    "f = open('Alice.txt','r', encoding = 'utf-8')\n",
    "\n",
    "#Read line by line\n",
    "alice = f.readlines()\n",
    "\n",
    "#Remove all \\n elements \n",
    "alRem = list(map(lambda s: s.strip(), alice ))\n",
    "\n",
    "#Check total number of elements\n",
    "print(\"Total number of list elements: \", len(alRem)) #3773 elements\n",
    "\n",
    "#List comprehension to remove empty strings\n",
    "alNoEmpStr = [i for i in alRem if i]\n",
    "\n",
    "#After removing empty strings, length of the list\n",
    "print(\"Total length of list after removing empty strings: \", len(alNoEmpStr)) #2815 elements\n",
    "\n",
    "#First sentence in Chapter One is 34th element and the last sentence is 2508th element\n",
    "#prepare a corpus based on these element numbers\n",
    "alCorpus = alNoEmpStr[34:2508]\n",
    "\n",
    "print(\"First two elements: \", alCorpus[0:2]) #First two elements in the list\n",
    "print(\"Last two elements: \", alCorpus[-2:]) #Last two elements in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3054\n",
      "Vocabulary Sample: [('the', 1), ('”', 2), ('and', 3), ('to', 4), ('a', 5), ('she', 6), ('of', 7), ('it', 8), ('said', 9), ('alice', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Now corpus is ready for CBOW Modeling\n",
    "tokenizer = text.Tokenizer() #Tokenizer instance\n",
    "\n",
    "#Fit the tokenizer object on the Alice corpus\n",
    "tokenizer.fit_on_texts(alCorpus)\n",
    "\n",
    "#create a dictionary\n",
    "word2idAl = tokenizer.word_index\n",
    "\n",
    "#Build corpus vocabulary\n",
    "\n",
    "#The PAD term is to pad context words to a fixed length if needed.\n",
    "word2idAl['PAD'] = 0\n",
    "\n",
    "#Exchange key and values and store in id2word for reverse mapping\n",
    "id2wordAl = {v:k for k, v in word2idAl.items()}\n",
    "\n",
    "#Each word is mapped to a number and stored as a list of list\n",
    "widsAl = [[word2idAl[w] for w in text.text_to_word_sequence(doc)] for doc in alCorpus]\n",
    "#print(wids)\n",
    "\n",
    "#Size of the vocabulary\n",
    "vocab_size_alice = len(word2idAl)\n",
    "\n",
    "#Embedding size\n",
    "embed_size = 100\n",
    "\n",
    "#context window size\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size_alice) #3054\n",
    "print('Vocabulary Sample:', list(word2idAl.items())[:10])\n",
    "\n",
    "#Check print(word2idAl) and print(id2wordAl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['alice', 'was', 'to', 'get'] -> Target (Y): beginning\n",
      "Context (X): ['was', 'beginning', 'get', 'very'] -> Target (Y): to\n",
      "Context (X): ['beginning', 'to', 'very', 'tired'] -> Target (Y): get\n",
      "Context (X): ['to', 'get', 'tired', 'of'] -> Target (Y): very\n",
      "Context (X): ['get', 'very', 'of', 'sitting'] -> Target (Y): tired\n",
      "Context (X): ['very', 'tired', 'sitting', 'by'] -> Target (Y): of\n",
      "Context (X): ['tired', 'of', 'by', 'her'] -> Target (Y): sitting\n",
      "Context (X): ['of', 'sitting', 'her', 'sister'] -> Target (Y): by\n",
      "Context (X): ['sitting', 'by', 'sister', 'on'] -> Target (Y): her\n",
      "Context (X): ['by', 'her', 'on', 'the'] -> Target (Y): sister\n",
      "Context (X): ['bank', 'and', 'having', 'nothing'] -> Target (Y): of\n"
     ]
    }
   ],
   "source": [
    "#Call the generate_context_word_pairs function created earlier\n",
    "\n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=widsAl, window_size=window_size, vocab_size=vocab_size_alice):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2wordAl[w] for w in x[0]], '-> Target (Y):', id2wordAl[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Neural Network based CBOW Model for Alice in Wonderland Corpus\n",
    "cbowAl = Sequential()\n",
    "\n",
    "#Input context words passed to embedding layer (initialised with random weights)\n",
    "cbowAl.add(Embedding(input_dim=vocab_size_alice, output_dim=embed_size, input_length=window_size*2))\n",
    "\n",
    "#Average out the word embeddings in lambda layer\n",
    "cbowAl.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "\n",
    "#Averaged context embedding is passed to a dense softmax layer which outputs the probabilities of each word in the vocabulary\n",
    "cbowAl.add(Dense(vocab_size_alice, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "cbowAl.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            305400    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3054)              308454    \n",
      "=================================================================\n",
      "Total params: 613,854\n",
      "Trainable params: 613,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cbowAl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of parameters in the embedding layer = 3054 (Vocabulary of alice corpus) * 100 dimensions = 305400\n",
    "#Number of parameters in the dense output layer = 3054 * 100 (dim) + 3054 (bias) = 308454 parameters\n",
    "#All 613854 parameters are trainable compared to 16080 parameters in earlier model of vocab 80 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 189024.347377792\n",
      "Epoch: 2 \tLoss: 228642.69925283268\n",
      "Epoch: 3 \tLoss: 232614.55808786396\n",
      "Epoch: 4 \tLoss: 236204.89445301797\n",
      "Epoch: 5 \tLoss: 243751.99776859814\n",
      "Epoch: 6 \tLoss: 247549.71646228596\n",
      "Epoch: 7 \tLoss: 249112.8310090748\n",
      "Epoch: 8 \tLoss: 249621.72604524484\n",
      "Epoch: 9 \tLoss: 251936.6662010532\n",
      "Epoch: 10 \tLoss: 253695.79349075432\n"
     ]
    }
   ],
   "source": [
    "#Train the model for 10 epochs. Each epoch takes at least 5 minutes. Total training time approximately 50 minutes\n",
    "#Loss is high (253695.793 after training for 10 epochs)\n",
    "LossAl = []\n",
    "EpochAl = []\n",
    "for epoch in range(1, 11):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=widsAl, window_size=window_size, vocab_size=vocab_size_alice):\n",
    "        i += 1\n",
    "        loss += cbowAl.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "    LossAl.append(loss)\n",
    "    EpochAl.append(epoch)\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['alice', 'was', 'to', 'get'] -> Target (Y): mock\n",
      "Context (X): ['was', 'beginning', 'get', 'very'] -> Target (Y): to\n",
      "Context (X): ['beginning', 'to', 'very', 'tired'] -> Target (Y): of\n",
      "Context (X): ['to', 'get', 'tired', 'of'] -> Target (Y): the\n",
      "Context (X): ['get', 'very', 'of', 'sitting'] -> Target (Y): one\n",
      "Context (X): ['very', 'tired', 'sitting', 'by'] -> Target (Y): of\n",
      "Context (X): ['tired', 'of', 'by', 'her'] -> Target (Y): one\n",
      "Context (X): ['of', 'sitting', 'her', 'sister'] -> Target (Y): and\n",
      "Context (X): ['sitting', 'by', 'sister', 'on'] -> Target (Y): her\n",
      "Context (X): ['by', 'her', 'on', 'the'] -> Target (Y): verses\n",
      "Context (X): ['bank', 'and', 'having', 'nothing'] -> Target (Y): of\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=widsAl, window_size=window_size, vocab_size=vocab_size_alice):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2wordAl[w] for w in x[0]], '-> Target (Y):', id2wordAl[np.argmax(cbowAl.predict([x,y]))])\n",
    "        \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "#Loss is high even after 10 epochs of training. Hence, the predicted word does not fit in properly\n",
    "\n",
    "#For second instance 'was begininning get very', the target center word 'to' is predicted correctly\n",
    "#For sixth instance 'very tired sitting by', the target center word 'of' is predicted correctly\n",
    "#For ninth instance 'sitting by sister on', the target center word 'her' is predicted correctly\n",
    "\n",
    "#Computation time for training the model is high for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss vs epoch for Alice Corpus\n",
    "epLoAlL = list(zip(EpochAl, LossAl)) #Zip the list\n",
    "epLoAlDF = pd.DataFrame(epLoAlL, columns = ['Epoch','Loss']) #Convert to a dataframe\n",
    "\n",
    "epLoAlDF.plot.line(x='Epoch', y= 'Loss', c='r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('CBOW Model for Alice in Wonderland corpus')\n",
    "plt.show()\n",
    "\n",
    "#Loss is increasing after 10 epochs. More training of the neural network is expected to reduce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3053, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>-1.444450</td>\n",
       "      <td>-1.679047</td>\n",
       "      <td>1.369818</td>\n",
       "      <td>-1.749136</td>\n",
       "      <td>1.169654</td>\n",
       "      <td>-1.117093</td>\n",
       "      <td>1.990417</td>\n",
       "      <td>-1.209428</td>\n",
       "      <td>1.109718</td>\n",
       "      <td>-1.529737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.694347</td>\n",
       "      <td>-1.397790</td>\n",
       "      <td>1.652419</td>\n",
       "      <td>-1.576714</td>\n",
       "      <td>-1.394550</td>\n",
       "      <td>0.884104</td>\n",
       "      <td>-1.962738</td>\n",
       "      <td>-1.950501</td>\n",
       "      <td>1.593528</td>\n",
       "      <td>-1.789765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.600773</td>\n",
       "      <td>-1.225610</td>\n",
       "      <td>1.170362</td>\n",
       "      <td>-0.733586</td>\n",
       "      <td>1.096177</td>\n",
       "      <td>-0.975140</td>\n",
       "      <td>2.770817</td>\n",
       "      <td>-1.270958</td>\n",
       "      <td>1.613718</td>\n",
       "      <td>-1.569972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969279</td>\n",
       "      <td>-1.599138</td>\n",
       "      <td>1.300984</td>\n",
       "      <td>-1.304014</td>\n",
       "      <td>-1.188717</td>\n",
       "      <td>1.050110</td>\n",
       "      <td>-1.654640</td>\n",
       "      <td>-1.252746</td>\n",
       "      <td>1.277909</td>\n",
       "      <td>-1.337074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-1.262331</td>\n",
       "      <td>-1.091118</td>\n",
       "      <td>1.393312</td>\n",
       "      <td>-0.968898</td>\n",
       "      <td>1.061633</td>\n",
       "      <td>-1.181811</td>\n",
       "      <td>2.019231</td>\n",
       "      <td>-1.810396</td>\n",
       "      <td>1.375010</td>\n",
       "      <td>-1.461204</td>\n",
       "      <td>...</td>\n",
       "      <td>1.400046</td>\n",
       "      <td>-1.332532</td>\n",
       "      <td>1.497840</td>\n",
       "      <td>-1.117057</td>\n",
       "      <td>-1.116535</td>\n",
       "      <td>1.393804</td>\n",
       "      <td>-1.268740</td>\n",
       "      <td>-1.310565</td>\n",
       "      <td>1.553882</td>\n",
       "      <td>-1.801036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-1.112520</td>\n",
       "      <td>-1.400369</td>\n",
       "      <td>1.073033</td>\n",
       "      <td>-1.245233</td>\n",
       "      <td>1.410770</td>\n",
       "      <td>-1.572835</td>\n",
       "      <td>2.268619</td>\n",
       "      <td>-1.201320</td>\n",
       "      <td>1.464215</td>\n",
       "      <td>-1.592534</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100950</td>\n",
       "      <td>-1.672802</td>\n",
       "      <td>1.236765</td>\n",
       "      <td>-0.772997</td>\n",
       "      <td>-1.048615</td>\n",
       "      <td>1.386138</td>\n",
       "      <td>-1.290731</td>\n",
       "      <td>-1.347803</td>\n",
       "      <td>0.959468</td>\n",
       "      <td>-1.423941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>-1.500956</td>\n",
       "      <td>-0.977607</td>\n",
       "      <td>0.744967</td>\n",
       "      <td>-1.182474</td>\n",
       "      <td>1.851719</td>\n",
       "      <td>-1.195286</td>\n",
       "      <td>2.921041</td>\n",
       "      <td>-1.174504</td>\n",
       "      <td>1.790422</td>\n",
       "      <td>-1.359003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301445</td>\n",
       "      <td>-1.079242</td>\n",
       "      <td>1.789387</td>\n",
       "      <td>-1.796575</td>\n",
       "      <td>-1.324456</td>\n",
       "      <td>1.499292</td>\n",
       "      <td>-1.353230</td>\n",
       "      <td>-1.283687</td>\n",
       "      <td>1.151912</td>\n",
       "      <td>-2.049520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "”   -1.444450 -1.679047  1.369818 -1.749136  1.169654 -1.117093  1.990417   \n",
       "and -0.600773 -1.225610  1.170362 -0.733586  1.096177 -0.975140  2.770817   \n",
       "to  -1.262331 -1.091118  1.393312 -0.968898  1.061633 -1.181811  2.019231   \n",
       "a   -1.112520 -1.400369  1.073033 -1.245233  1.410770 -1.572835  2.268619   \n",
       "she -1.500956 -0.977607  0.744967 -1.182474  1.851719 -1.195286  2.921041   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       "”   -1.209428  1.109718 -1.529737  ...  1.694347 -1.397790  1.652419   \n",
       "and -1.270958  1.613718 -1.569972  ...  0.969279 -1.599138  1.300984   \n",
       "to  -1.810396  1.375010 -1.461204  ...  1.400046 -1.332532  1.497840   \n",
       "a   -1.201320  1.464215 -1.592534  ...  1.100950 -1.672802  1.236765   \n",
       "she -1.174504  1.790422 -1.359003  ...  1.301445 -1.079242  1.789387   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "”   -1.576714 -1.394550  0.884104 -1.962738 -1.950501  1.593528 -1.789765  \n",
       "and -1.304014 -1.188717  1.050110 -1.654640 -1.252746  1.277909 -1.337074  \n",
       "to  -1.117057 -1.116535  1.393804 -1.268740 -1.310565  1.553882 -1.801036  \n",
       "a   -0.772997 -1.048615  1.386138 -1.290731 -1.347803  0.959468 -1.423941  \n",
       "she -1.796575 -1.324456  1.499292 -1.353230 -1.283687  1.151912 -2.049520  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word embeddings for the vocabulary\n",
    "\n",
    "weightsAl = cbowAl.get_weights()[0] #Word embedding of PAD\n",
    "weightsAl = weightsAl[1:] #Exclude word embedding of PAD\n",
    "print(weightsAl.shape) # 3053 (3054-1) vocabulary \n",
    "\n",
    "#Convert the weights to a dataframe for each of the word\n",
    "#A single row shows the word embedding done in 100 dimensions by CBOW model for the alice corpus\n",
    "\n",
    "#Shape of dataframe (3053,100)\n",
    "\n",
    "aliceDF = pd.DataFrame(weightsAl, index=list(id2wordAl.values())[1:])\n",
    "aliceDF.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
